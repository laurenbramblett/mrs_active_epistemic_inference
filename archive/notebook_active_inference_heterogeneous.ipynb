{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qbr5kx\\OneDrive - University of Virginia\\Desktop\\UVA\\PhD Scratch\\Active_Epistemic_Inference\\particle_filter_robot/\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from palettable.colorbrewer.qualitative import Set1_9\n",
    "import os, math\n",
    "from datetime import datetime\n",
    "from IPython.display import HTML\n",
    "\n",
    "pwd = os.path.abspath('') + \"/\"\n",
    "print(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def log_stable(x):\n",
    "    \"\"\"Compute log values for each sets of scores in x.\"\"\"\n",
    "    return np.log(x + np.exp(-16))\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, state, agentType, velocity_options, heading_options, goals):\n",
    "        self.state = state #This is the true state of the robot\n",
    "        self.type = agentType\n",
    "        self.velocity_options = velocity_options\n",
    "        self.heading_options = heading_options\n",
    "        self.goal_prior = np.ones(goals.shape[0])*1/goals.shape[0]\n",
    "        self.likelihood = self.goal_prior\n",
    "        self.posterior = self.goal_prior\n",
    "        self.max_iterations = 100\n",
    "\n",
    "    def update(self, heading, velocity, dt):\n",
    "        \"\"\"Update agent's position based on chosen velocity and heading.\"\"\"\n",
    "        self.state[0] += velocity * np.cos(heading) * dt\n",
    "        self.state[1] += velocity * np.sin(heading) * dt\n",
    "        self.state[2] = heading\n",
    "    \n",
    "    def get_measurements(self, observations, goals):\n",
    "        \"\"\"Return measurements of agent's position given observations.\"\"\"\n",
    "        for key in observations:\n",
    "            if key['type'] == 'A':\n",
    "                distance = np.linalg.norm(self.state-key['obs'])\n",
    "                self.measured_states[key['id']] = distance\n",
    "            \n",
    "\n",
    "class SimEnv:\n",
    "    def __init__(self, num_agents, goals, num_timesteps, dt):\n",
    "        self.num_agents = num_agents\n",
    "        self.num_goals = goals.shape[0]\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.dt = dt\n",
    "    def calculate_kl_divergence(p, q):\n",
    "        \"\"\"Calculate KL divergence between two probability distributions.\"\"\"\n",
    "        return np.sum(p * np.log(p / q + np.exp(-16)))\n",
    "    def calculate_shannon_entropy(p):\n",
    "        \"\"\"Calculate Shannon entropy of a probability distribution.\"\"\"\n",
    "        return -np.sum(p * np.log(p))\n",
    "    def calculate_likelihood(self, agent):\n",
    "        \"\"\"Calculate likelihood of agent's measurements given each goal.\"\"\"\n",
    "        for id in range(self.num_goals):\n",
    "            if agent.type == 'A':\n",
    "                distance = np.linalg.norm(agent.measured_states[id]-agent.state)\n",
    "                agent.likelihood[id] = np.exp(-distance)\n",
    "            else:\n",
    "                azimuth = np.arctan2(agent.measured_states[id][1]-agent.state[1],agent.measured_states[id][0]-agent.state[0])\n",
    "                agent.likelihood[id] = np.exp(-azimuth)\n",
    "        return agent.likelihood\n",
    "                \n",
    "\n",
    "    \n",
    "class agentSystem:\n",
    "    def __init__(self, agents):\n",
    "        self.agents = agents\n",
    "    def update(self,agents):\n",
    "        self.agents = agents\n",
    "    def simulate_observations(self, agents):\n",
    "        \"\"\"Simulate noisy observations of agent positions given agents structure.\"\"\"\n",
    "        observations = {}\n",
    "        for id1,agent1 in enumerate(agents):\n",
    "            id1Obs = []\n",
    "            for id2,agent2 in enumerate(agents):\n",
    "                if agent1.type == 'A':\n",
    "                    distance = np.linalg.norm(agent2.state-agent1.state)\n",
    "                    observation = np.linalg.norm(agent2.state-agent1.state) + np.random.normal(0, np.sqrt(distance),1)\n",
    "                else:\n",
    "                    azimuth = np.arctan2(agent2.state[1]-agent1.state[1],agent2.state[0]-agent1.state[0])\n",
    "                    observation = azimuth + np.random.uniform(0, azimuth,1)\n",
    "                id1Obs.append(observation)\n",
    "            observations[id1] = {'id': id1, 'obs': id1Obs, 'type': agent1.type}\n",
    "        return observations\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = Set1_9.mpl_colors\n",
    "# Re-define the environment and simulation parameters here\n",
    "goals = np.array([[1, 1], [10, 10]], dtype=float)  # Goal positions\n",
    "agent_positions = np.array([[9, 1, np.pi/2], [1, 9, np.pi/2]], dtype=float)  # Initial agent positions\n",
    "agent_types = ['A', 'A']  # Agent types for differentiating their types\n",
    "velocity_options = [0, 0.5, 1.0]  # Velocity options for the agents\n",
    "heading_options = [0, np.pi/4, np.pi, 3*np.pi/4]  # Heading options (radians)\n",
    "observation_error_std = 3.0  # Observation noise standard deviation\n",
    "max_iterations = 100  # Maximum number of iterations\n",
    "num_agents = agent_positions.shape[0]  # Number of agents\n",
    "\n",
    "agents = []  # List to store agent objects\n",
    "agent_dict = {}  # Dictionary to store agent system objects for each agent\n",
    "# Initialize agents\n",
    "for i in range(num_agents):\n",
    "    agents.append(Agent(np.zeros((num_agents, 1)), agent_positions[i], agent_types[i], velocity_options, heading_options, goals))\n",
    "# Initialize simulation environment\n",
    "env = SimEnv(num_agents, goals, max_iterations, 0.1)  # Initialize simulation environment\n",
    "# Initialize agent system\n",
    "for i in range(num_agents):\n",
    "    agent_dict[i] = {'ID': i, 'Agent': agents[i], 'AgentSystem': agentSystem(agents)}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'id': 0, 'obs': [array([0.]), array([10.30829063])], 'type': 'A'}, 1: {'id': 1, 'obs': [array([9.59780487]), array([0.])], 'type': 'A'}}\n",
      "{0: {'id': 0, 'obs': [array([0.]), array([6.47196933])], 'type': 'A'}, 1: {'id': 1, 'obs': [array([14.3915028]), array([0.])], 'type': 'A'}}\n"
     ]
    }
   ],
   "source": [
    "# Simulate noisy observation of the other agent's position\n",
    "for id in agent_dict.keys():\n",
    "    agent_sys = agent_dict[id]['AgentSystem']\n",
    "    observations = agent_sys.simulate_observations(agent_sys.agents)\n",
    "    measurements = get_measurements(observations)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for goal in goals:\n",
    "    # Initialize a score for how attainable each goal seems for both agents\n",
    "    goal_scores = []\n",
    "            \n",
    "    for velocity in velocity_options:\n",
    "        for heading in heading_options:\n",
    "            predicted_position = predict_agent_position(agent_positions[agent_id], velocity, heading)\n",
    "\n",
    "            # Estimate how both agents are aligned with reaching the current goal\n",
    "            distance_to_goal = np.linalg.norm(predicted_position - goal)\n",
    "            distance_other_to_goal = np.linalg.norm(other_agent_observed_position - goal)\n",
    "\n",
    "            # Use the sum of both distances as a simple score for this action's alignment with the goal\n",
    "            goal_alignment_score = distance_to_goal + distance_other_to_goal\n",
    "            \n",
    "            goal_scores.append((goal_alignment_score, velocity, heading))\n",
    "\n",
    "    # Choose the action (for the current goal) that minimizes the combined distance\n",
    "    best_action_for_goal = min(goal_scores, key=lambda x: x[0])\n",
    "\n",
    "    # Update best action if this goal is more attainable than previous best\n",
    "    if best_action_for_goal[0] < best_score:\n",
    "        best_score = best_action_for_goal[0]\n",
    "        best_action = best_action_for_goal[1], best_action_for_goal[2]\n",
    "\n",
    "return best_action\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ani.save(pwd + \"videos/two_goals_choice\" + current_time + \".mp4\", writer='ffmpeg', fps=3, dpi=300)\n",
    "# print(\"Image saved as: \", pwd + \"videos/two_goals_choice\" + current_time + \".mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decision(agent_id, agent_positions):\n",
    "    \"\"\"Agent decision-making based on active inference to encourage convergence on a shared goal.\"\"\"\n",
    "    best_action = None\n",
    "    best_score = np.inf\n",
    "\n",
    "    # Simulate noisy observation of the other agent's position\n",
    "    for id in agent_dict.keys():\n",
    "        print(agent_dict[id])\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    for goal in goals:\n",
    "        # Initialize a score for how attainable each goal seems for both agents\n",
    "        goal_scores = []\n",
    "                \n",
    "        for velocity in velocity_options:\n",
    "            for heading in heading_options:\n",
    "                predicted_position = predict_agent_position(agent_positions[agent_id], velocity, heading)\n",
    "\n",
    "                # Estimate how both agents are aligned with reaching the current goal\n",
    "                distance_to_goal = np.linalg.norm(predicted_position - goal)\n",
    "                distance_other_to_goal = np.linalg.norm(other_agent_observed_position - goal)\n",
    "\n",
    "                # Use the sum of both distances as a simple score for this action's alignment with the goal\n",
    "                goal_alignment_score = distance_to_goal + distance_other_to_goal\n",
    "                \n",
    "                goal_scores.append((goal_alignment_score, velocity, heading))\n",
    "\n",
    "        # Choose the action (for the current goal) that minimizes the combined distance\n",
    "        best_action_for_goal = min(goal_scores, key=lambda x: x[0])\n",
    "\n",
    "        # Update best action if this goal is more attainable than previous best\n",
    "        if best_action_for_goal[0] < best_score:\n",
    "            best_score = best_action_for_goal[0]\n",
    "            best_action = best_action_for_goal[1], best_action_for_goal[2]\n",
    "    \n",
    "    return best_action\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ani.save(pwd + \"videos/two_goals_choice\" + current_time + \".mp4\", writer='ffmpeg', fps=3, dpi=300)\n",
    "# print(\"Image saved as: \", pwd + \"videos/two_goals_choice\" + current_time + \".mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.99954602e-01 4.53978687e-05]\n",
      "[0.68684237 0.31315763]\n",
      "[0.66525402 0.33474598]\n",
      "[0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "prior = np.array([0.5,0.5])\n",
    "obs1 = np.array([10,20])\n",
    "obs2 = np.array([np.pi/4,np.pi/2])\n",
    "likelihood1 = softmax(-obs1)\n",
    "print(likelihood1)\n",
    "likelihood2 = softmax(-obs2)\n",
    "print(likelihood2)\n",
    "joint = likelihood1 * likelihood2 \n",
    "posterior = softmax(joint + np.log(prior))\n",
    "print(posterior)\n",
    "\n",
    "print(softmax([0,0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the plotting arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize figure for plotting\n",
    "fig, ax = plt.subplots()\n",
    "plt.xlim(-5, 15)\n",
    "plt.ylim(-5, 15)\n",
    "# Paths (with smaller line width)\n",
    "agent_paths = [ax.plot([], [], 'o-', markersize=3, linewidth=1, alpha=0.5, color=cmap[i])[0] for i in range(2)]\n",
    "# Current positions (with larger markers)\n",
    "agent_markers = [ax.plot([], [], 'o', markersize=10, color=cmap[i])[0] for i in range(2)]\n",
    "goal_plots = [ax.plot(goal[0], goal[1], 'x', markersize=10, color='purple')[0] for goal in goals]  # Plot goals\n",
    "\n",
    "\n",
    "def init():\n",
    "    \"\"\"Initialize the background of the plot.\"\"\"\n",
    "    for agent_path, agent_marker in zip(agent_paths, agent_markers):\n",
    "        agent_path.set_data([], [])\n",
    "        agent_marker.set_data([], [])\n",
    "    return agent_paths + agent_markers\n",
    "\n",
    "def update(frame):\n",
    "    \"\"\"Update the plot for each frame.\"\"\"    \n",
    "    decisions = [make_decision(agent_id, agent_positions) for agent_id in range(2)]\n",
    "    \n",
    "    # Update positions based on decisions\n",
    "    for agent_id, (velocity, heading) in enumerate(decisions):\n",
    "        dx = velocity * np.cos(heading)\n",
    "        dy = velocity * np.sin(heading)\n",
    "        agent_positions[agent_id] += np.array([dx, dy])\n",
    "    \n",
    "    # Update plot data\n",
    "    for agent_id, (agent_path, agent_marker) in enumerate(zip(agent_paths, agent_markers)):\n",
    "        xdata, ydata = agent_path.get_data()\n",
    "        xnew, ynew = agent_positions[agent_id]\n",
    "        xdata = np.append(xdata, xnew)\n",
    "        ydata = np.append(ydata, ynew)\n",
    "        agent_path.set_data(xdata, ydata)\n",
    "        agent_marker.set_data(xnew, ynew)\n",
    "    \n",
    "    return agent_paths + agent_markers\n",
    "\n",
    "def run_simulation(max_iterations=100):\n",
    "    \"\"\"Run the simulation until both agents converge to the same goal or max iterations reached.\"\"\"\n",
    "    current_positions = np.copy(agent_positions)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        decisions = [make_decision(agent_id, current_positions) for agent_id in range(2)]\n",
    "        \n",
    "        # Update agent positions based on their decisions\n",
    "        for agent_id, (velocity, heading) in enumerate(decisions):\n",
    "            dx = velocity * np.cos(heading)\n",
    "            dy = velocity * np.sin(heading)\n",
    "            current_positions[agent_id] += np.array([dx, dy])\n",
    "        \n",
    "        # Check if agents have converged to the same goal\n",
    "        distances_to_goals = [np.linalg.norm(goals - pos, axis=1) for pos in current_positions]\n",
    "        goal_reached_by_agents = [np.argmin(distances) for distances in distances_to_goals]\n",
    "        distances_to_selected_goal = [np.min(distances) for distances in distances_to_goals]\n",
    "        \n",
    "        if (np.array(distances_to_selected_goal)<1).all():\n",
    "            print(distances_to_selected_goal)\n",
    "            print(f\"Agents have converged to Goal {goal_reached_by_agents[0]} after {iteration + 1} iterations.\")\n",
    "            return current_positions, goal_reached_by_agents[0], iteration\n",
    "\n",
    "    print(\"Agents did not converge to the same goal within the maximum iterations.\")\n",
    "    return current_positions, None, iteration\n",
    "\n",
    "# Run the simulation\n",
    "final_positions, goal_converged, num_frames = run_simulation()\n",
    "# Create animation\n",
    "ani = FuncAnimation(fig, update, frames=range(num_frames), init_func=init, blit=True, repeat=True)\n",
    "\n",
    "# Save the animation as a video\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "HTML(ani.to_html5_video()) # Use an interactive backend for animation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
