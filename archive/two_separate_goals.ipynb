{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.88656219e-01 9.43555193e-02 1.52597027e-02 3.29630184e-04\n",
      " 1.16243643e-03 2.36492038e-04]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import itertools\n",
    "import numpy as np\n",
    "def custom_cdist(x1, x2, types):\n",
    "    \"\"\"\n",
    "    Compute the pairwise distance between rows of x1 and rows of x2 based on measurement types.\n",
    "\n",
    "    Args:\n",
    "        x1 (torch.Tensor): A tensor of shape (m, d)\n",
    "        x2 (torch.Tensor): A tensor of shape (n, d)\n",
    "        types (list): A list of measurement types for each pair of rows.\n",
    "        p (float): The norm degree. Default is 2 for Euclidean distance.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (m, n) with the pairwise distances.\n",
    "    \"\"\"\n",
    "    assert len(types) == x1.shape[0], \"Length of types must match number of rows in x1\"\n",
    "    \n",
    "    # Expand x1 and x2 to compute pairwise distances\n",
    "    x1_expanded = x1.unsqueeze(1)  # shape: (m, 1, d)\n",
    "    x2_expanded = x2.unsqueeze(0)  # shape: (1, n, d)\n",
    "    \n",
    "    # Compute the difference between each pair of points\n",
    "    diff = x1_expanded - x2_expanded  # shape: (m, n, d)\n",
    "    \n",
    "    distances = torch.zeros((x1.shape[0], x2.shape[0]))\n",
    "    \n",
    "    for i, t in enumerate(types):\n",
    "        if t in ['A', 'self']:\n",
    "            # For Euclidean distance\n",
    "            dist = torch.sqrt(torch.sum(diff[i] ** 2, dim=-1))\n",
    "        elif t in ['B']:\n",
    "            # For azimuth distance\n",
    "            goal_vectors = x2 - x1[i, :2]\n",
    "            goal_azimuths = torch.atan2(goal_vectors[:, 1], goal_vectors[:, 0])\n",
    "            observed_azimuth = torch.atan2(x2[:, 1] - x1[i, 1], x2[:, 0] - x1[i, 0])\n",
    "            relative_azimuths = torch.abs((goal_azimuths - observed_azimuth + np.pi) % (2 * np.pi) - np.pi)\n",
    "            dist = 1.0 / 8 * torch.exp(-relative_azimuths / np.pi)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown type\")\n",
    "        \n",
    "        distances[i] = dist\n",
    "    \n",
    "    return distances\n",
    "\n",
    "def calculate_joint_goal_probabilities_with_configs(agent_positions, goal_positions, obs_types, reward_configurations):\n",
    "    \"\"\"\n",
    "    Calculate the joint goal probabilities for any number of agents and goals,\n",
    "    applying a reward to specified configurations.\n",
    "\n",
    "    Parameters:\n",
    "    - agent_positions (tensor): Tensor of shape [num_agents, 2] representing the positions of agents.\n",
    "    - goal_positions (tensor): Tensor of shape [num_goals, 2] representing the positions of goals.\n",
    "    - reward_configurations (list of tuples): List of configurations to reward. Each configuration is a tuple of goal indices.\n",
    "    - reward_factor (float): Factor by which to increase the probability of specified configurations.\n",
    "\n",
    "    Returns:\n",
    "    - joint_probabilities (tensor): Tensor of shape [num_goals, num_goals, ...] (num_agents times) representing the joint probabilities.\n",
    "    \"\"\"\n",
    "    num_agents = agent_positions.shape[0]\n",
    "    num_goals = goal_positions.shape[0]\n",
    "\n",
    "    # Calculate distances between agents and goals\n",
    "    distances = custom_cdist(agent_positions, goal_positions, obs_types)\n",
    "    # distances = torch.cdist(agent_positions, goal_positions, p=2)  # Calculate Euclidean distances (L2 norm\n",
    "    # Convert distances to probabilities using softmax\n",
    "    probabilities = torch.softmax(-distances, dim=1)  # Apply softmax along the goal dimension\n",
    "\n",
    "    # Initialize joint probabilities as a tensor of ones with the appropriate shape\n",
    "    joint_probabilities = torch.ones([num_goals] * num_agents)\n",
    "\n",
    "    # Calculate joint probabilities\n",
    "    for i in range(num_agents):\n",
    "        shape = [1] * num_agents\n",
    "        shape[i] = num_goals\n",
    "        joint_probabilities = joint_probabilities * probabilities[i].view(shape)\n",
    "\n",
    "    # Only return the specified configurations\n",
    "    likelihood = torch.zeros(len(reward_configurations), dtype=torch.float64)\n",
    "    for id, config in enumerate(reward_configurations):\n",
    "        likelihood[id] = joint_probabilities[config].prod()\n",
    "\n",
    "    # Normalize the joint probabilities\n",
    "    likelihood /= likelihood.sum()\n",
    "\n",
    "    return likelihood\n",
    "\n",
    "# Example usage\n",
    "# Define the positions of agents and goals (arbitrary example positions)\n",
    "agent_positions = torch.tensor([[1, 2], [4, 4], [5, 6]],dtype=float)  # 3 agents\n",
    "goal_positions = torch.tensor([[2, 3], [4, 5], [6, 6]],dtype=float)  # 3 goals\n",
    "obs_types = ['A', 'A', 'A']\n",
    "\n",
    "# Define reward configurations\n",
    "configurations = [(0, 0, 0), (0, 1, 0)]  # Return normalized likelihoods for (G1, G1, G1) and (G1, G2, G1)\n",
    "# Generate all permutations\n",
    "tuple_elements = [i for i in range(goal_positions.shape[0])]\n",
    "configurations = list(itertools.permutations(tuple_elements))\n",
    "\n",
    "# Calculate joint goal probabilities with specific rewards\n",
    "joint_probabilities_with_specific_rewards = calculate_joint_goal_probabilities_with_configs(agent_positions, goal_positions, obs_types, configurations)\n",
    "\n",
    "# Print the joint probabilities tensor with specific rewards\n",
    "print(joint_probabilities_with_specific_rewards.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35009169 0.32495415 0.32495415]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def custom_cdist(x1, x2, types, max_distance=30.0):\n",
    "    \"\"\"\n",
    "    Compute the pairwise distance between rows of x1 and rows of x2 based on measurement types.\n",
    "\n",
    "    Args:\n",
    "        x1 (np.ndarray): An array of shape (m, d)\n",
    "        x2 (np.ndarray): An array of shape (n, d)\n",
    "        types (list): A list of measurement types for each pair of rows.\n",
    "    Returns:\n",
    "        np.ndarray: An array of shape (m, n) with the pairwise distances.\n",
    "    \"\"\"\n",
    "    assert len(types) == x1.shape[0], \"Length of types must match number of rows in x1\"\n",
    "\n",
    "    distances = np.zeros((x1.shape[0], x2.shape[0]), dtype=float)\n",
    "    \n",
    "    for i, t in enumerate(types):\n",
    "        if t == 's': # Self-observation\n",
    "            depth_multiplier = max(2.0, 2.0 / np.min(x1[i] - x2))\n",
    "            dist = depth_multiplier * np.exp(-np.linalg.norm(x1[i] - x2, axis=-1) / max_distance)\n",
    "        elif t == 'A':\n",
    "            dist = np.exp(-np.linalg.norm(x1[i] - x2, axis=-1) / max_distance)\n",
    "        elif t == 'B':\n",
    "            goal_vectors = x2 - x1[i, :2]\n",
    "            goal_azimuths = np.arctan2(goal_vectors[:, 1], goal_vectors[:, 0])\n",
    "            observed_azimuth = np.arctan2(x2[:, 1] - x1[i, 1], x2[:, 0] - x1[i, 0])\n",
    "            relative_azimuths = np.abs((goal_azimuths - observed_azimuth + np.pi) % (2 * np.pi) - np.pi)\n",
    "            dist = 1.0 / 8 * np.exp(-relative_azimuths / np.pi)\n",
    "        else:\n",
    "            dist = np.zeros(x2.shape[0], dtype=x1.dtype)\n",
    "        \n",
    "        distances[i] = dist\n",
    "    \n",
    "    return distances\n",
    "\n",
    "def calculate_joint_goal_probs(agent_poses, goals, predict_types, reward_configs, max_distance=30.0):\n",
    "    \"\"\"\n",
    "    Calculate the joint goal probabilities for any number of agents and goals,\n",
    "    applying a reward to specified configurations.\n",
    "\n",
    "    Parameters:\n",
    "    - agent_poses (np.ndarray): Array of shape [num_agents, 2] representing the positions of agents.\n",
    "    - goals (np.ndarray): Array of shape [num_goals, 2] representing the positions of goals.\n",
    "    - predict_types (list): List of types for prediction\n",
    "    - reward_configs (list of tuples): List of configurations to reward. Each configuration is a tuple of goal indices.\n",
    "    \n",
    "    Returns:\n",
    "    - joint_probabilities (np.ndarray): Array representing the joint probabilities.\n",
    "    \"\"\"\n",
    "    num_agents = agent_poses.shape[0]\n",
    "    num_goals = goals.shape[0]\n",
    "\n",
    "    # Calculate distances between agents and goals\n",
    "    distances = custom_cdist(agent_poses, goals, predict_types, max_distance)\n",
    "\n",
    "    # Convert distances to probabilities using softmax\n",
    "    probabilities = np.exp(distances) / np.exp(distances).sum(axis=1, keepdims=True)  # Apply softmax along the goal dimension\n",
    "\n",
    "    # Initialize joint probabilities as an array of ones with the appropriate shape\n",
    "    joint_probabilities = np.ones([num_goals] * num_agents, dtype=probabilities.dtype)\n",
    "\n",
    "    # Calculate joint probabilities\n",
    "    for i in range(num_agents):\n",
    "        joint_probabilities *= probabilities[i].reshape([num_goals if j == i else 1 for j in range(num_agents)]) \n",
    "\n",
    "    # Only return the specified configurations\n",
    "    likelihood = np.array([joint_probabilities[tuple(config)] for config in reward_configs], dtype=np.float64)\n",
    "\n",
    "    # Normalize the joint probabilities\n",
    "    likelihood /= likelihood.sum()\n",
    "\n",
    "    return likelihood\n",
    "\n",
    "# Example usage:\n",
    "agent_poses = np.array([[1, 2], [3, 4]])\n",
    "goals = np.array([[5, 6], [7, 8], [9, 10]])\n",
    "predict_types = ['A', 'B']\n",
    "reward_configs = [(0, 1), (1, 2), (1,1)]\n",
    "\n",
    "likelihood = calculate_joint_goal_probs(agent_poses, goals, predict_types, reward_configs)\n",
    "print(likelihood)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
