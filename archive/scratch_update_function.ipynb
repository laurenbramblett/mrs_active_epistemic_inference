{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final positions of robots: [[ 0.2  0.2]\n",
      " [10.2 10.2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "max_steps = 10\n",
    "tolerance = 0.1\n",
    "# Define the environment\n",
    "class Environment:\n",
    "    def __init__(self, size, robot_positions, goals):\n",
    "        self.size = size\n",
    "        self.robot_positions = robot_positions\n",
    "        self.goals = goals\n",
    "\n",
    "# Define the belief states\n",
    "class BeliefState:\n",
    "    def __init__(self, positions, velocities, goals):\n",
    "        self.positions = positions\n",
    "        self.velocities = velocities\n",
    "        self.goals = goals\n",
    "\n",
    "# Define the cost function\n",
    "def cost_function(position, goal, other_robot_position):\n",
    "    direct_distance = np.linalg.norm(position - goal)\n",
    "    arc_distance = np.linalg.norm(position - other_robot_position)\n",
    "    return direct_distance - arc_distance\n",
    "\n",
    "# Prediction step\n",
    "def predict_next_state(belief_state, action):\n",
    "    # Update positions based on action\n",
    "    new_positions = belief_state.positions + action\n",
    "    return BeliefState(new_positions, belief_state.velocities, belief_state.goals)\n",
    "\n",
    "# Update step\n",
    "def update_belief(observation, belief_state):\n",
    "    # Update the belief based on the observation\n",
    "    new_positions = observation\n",
    "    return BeliefState(new_positions, belief_state.velocities, belief_state.goals)\n",
    "\n",
    "# Policy evaluation\n",
    "def evaluate_policy(belief_state, actions):\n",
    "    min_expected_free_energy = float('inf')\n",
    "    best_action = None\n",
    "    for action in actions:\n",
    "        predicted_state = predict_next_state(belief_state, action)\n",
    "        cost = cost_function(predicted_state.positions[0], belief_state.goals[0], belief_state.positions[1])\n",
    "        if cost < min_expected_free_energy:\n",
    "            min_expected_free_energy = cost\n",
    "            best_action = action\n",
    "    return best_action\n",
    "\n",
    "# Action selection and execution\n",
    "def execute_policy(environment, belief_state, actions):\n",
    "    for step in range(max_steps):\n",
    "        best_action = evaluate_policy(belief_state, actions)\n",
    "        new_positions = environment.robot_positions + best_action\n",
    "        environment.robot_positions = new_positions\n",
    "        belief_state = update_belief(new_positions, belief_state)\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(environment.robot_positions - environment.goals) < tolerance:\n",
    "            break\n",
    "    return environment.robot_positions\n",
    "\n",
    "# Example usage\n",
    "env = Environment(size=10, robot_positions=np.array([[0, 0], [10, 10]]), goals=np.array([[5, 5], [5, 0]]))\n",
    "belief_state = BeliefState(positions=env.robot_positions, velocities=np.array([[0, 0], [0, 0]]), goals=env.goals)\n",
    "actions = [np.array([0.1, 0.1]), np.array([0.1, -0.1]), np.array([-0.1, 0.1]), np.array([-0.1, -0.1])]\n",
    "\n",
    "final_positions = execute_policy(env, belief_state, actions)\n",
    "print(\"Final positions of robots:\", final_positions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
